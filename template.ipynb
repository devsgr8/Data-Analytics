{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Last amended:09th Sep, 2022\n",
    "#  Myfolder: /home/ashok/Documents/spark\n",
    "# Ref:\n",
    "# Tutorials (slightly dated):\n",
    "#      https://changhsinlee.com/pyspark-dataframe-basics/\n",
    "#      https://www.analyticsvidhya.com/blog/2016/10/spark-dataframe-and-operations/\n",
    "# Cheat Sheet\n",
    "#      https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_SQL_Cheat_Sheet_Python.pdf\n",
    "\n",
    "#  Objectives:\n",
    "#           Dataframe operations in spark cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyspark APIs<br>\n",
    "> i)  [DataFrame APIs](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#dataframe-apis)<br>\n",
    ">> df.select(columnName).where(colObject > 30).orderBy(desc(columnName))<br>\n",
    ">> df.select(columnName).where(\"colName > 30\").orderBy(desc(columnName))<br>\n",
    "\n",
    "> ii) [Column APIs](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#column-apis)<br>\n",
    ">> df.select(df.age.isNull())<br>\n",
    ">> df.select(df[\"age\"].isNull())<br>\n",
    ">> df.select(col(\"age\").isNull())<br>\n",
    "\n",
    "> iii)[Data Tyoes](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#data-types)<br>\n",
    "> iv) [Functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#functions)<br>\n",
    ">> df.select(sum(\"age\"))<br>\n",
    ">> df.select(sum(col(booleanColumn).cast(\"int\")))<br>\n",
    ">> <u>but you must import the functions</u>\n",
    "\n",
    "> v)  [Grouping](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#grouping)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A. Initial operations:\n",
    "1.0 Start hadoop in a terminal:\n",
    "\n",
    "            ./allstart.sh\n",
    "            OR\n",
    "            ./quick_allstart.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer files to hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Transfer data file 'blackfridayless.csv' to hadoop\n",
    "#     Linux File folder:  /cdata/misc_datasets/black_friday\n",
    "#     In Hadoop first make a folder: /user/ashok/datadir \n",
    "#     and then transfer the file 'blackfridayless.csv' to \n",
    "#     this folder: /user/ashok/datadir\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "cd ~\n",
    "hdfs dfs -rm -f -r  /user/ashok/datadir\n",
    "hdfs dfs -mkdir /user/ashok/datadir\n",
    "hdfs dfs -put /cdata/misc_datasets/black_friday/blackfridayless.csv  /user/ashok/datadir\n",
    "hdfs dfs -ls /user/ashok/datadir\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "!cd ~\n",
    "!hdfs dfs -rm -f -r  /user/ashok/datadir\n",
    "!hdfs dfs -mkdir /user/ashok/datadir\n",
    "!hdfs dfs -put /cdata/misc_datasets/black_friday/blackfridayless.csv  /user/ashok/datadir\n",
    "!hdfs dfs -ls /user/ashok/datadir\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set jupyter notebook options\n",
    "Start pyspark with jupyter notebook interface. There is no need to create SparkContext and Spark session. pyspark creates them when starting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Display multiple outputs from a cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 Increase cell width to display wide columnar output\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the csv file from hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Read file 'blackfridayless.csv' from hadoop\n",
    "\n",
    "# 2.0 What is the URL of folder in hadoop where blackfriday file existshaving the file?\n",
    "#     url: \"http://localhost:9000/<folderPath>\"\n",
    "\n",
    "\n",
    "URL_of_folder= \"hdfs://localhost:9000/user/ashok/datadir/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Read the file blackfridayless.csv. Takes time.\n",
    "#      Use 'spark.read.csv' session object to read file:\n",
    "#       Here is reading template:\n",
    "\n",
    "blackfriday = spark.read.csv(\n",
    "                             path = URL_of_folder + \"blackfridayless.csv\" ,\n",
    "                             inferSchema = True,                 # True or False\n",
    "                             header = True,                      # True or False\n",
    "                             sep = \",\",                         # Which one: , ;, | etc\n",
    "                             ignoreLeadingWhiteSpace = True,     # True or False\n",
    "                             ignoreTrailingWhiteSpace = True    # True of False\n",
    "    \n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Show five rows of data:\n",
    "\n",
    "blackfriday.head()\n",
    "blackfriday.show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 Show data columns\n",
    "\n",
    "blackfriday.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4 Show dtypes:\n",
    "\n",
    "blackfriday.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.5 Print schema of blackfriday:\n",
    "\n",
    "blackfriday.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.0 Describe the statistics of data, few columns at a time:\n",
    "blackfriday.select(\"gender\", \"age\").describe()\n",
    "blackfriday.select(\"gender\", \"age\").describe().show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Count How many distinct userids are there \n",
    "#     Use distinct() and count()\n",
    "\n",
    "blackfriday.select('userid').count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Count how many distinct age-groups exist\n",
    "\n",
    "blackfriday.select('age').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.0 How many null values occur in each column\n",
    "\n",
    "from pyspark.sql.functions import isnan, isnull,col, sum, max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1\n",
    "for i in blackfriday.columns:\n",
    "    blackfriday.select(sum(col(i).isNull().alias(\"nullcol\").cast(\"int\")).alias(i)).show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These columns have null values. Most probably it means that there is no sub-category or sub-categories present. <br>\n",
    "How would you plan to fill them?<br>\n",
    "productCat1 :  0 <br>\n",
    "productCat2 :  31429 <br>\n",
    "productCat3 :  70100 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.0 Get a list of all integer columns and string columns\n",
    "#     Use list comprhension along with dtypes:\n",
    "\n",
    "[ i[0]  for i in blackfriday.dtypes   if i[1] == 'int']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Display maximum of productCat2 and productCat3\n",
    "#     Use select() along with 'max' function\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 Find minimum and max values of 'occupation' column\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3 Fill null values in productCat2 and productCat3 with 999\n",
    "#     Use df.na.fill({})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.0 Transform spark dataframe to pandas dataframe:\n",
    "#     Use df.toPandas()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Show a value count of levels of column 'cityCategory':\n",
    "#      Use groupby and count\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.0 Perform a stratified sampling of data.\n",
    "#      Stratified sampling be by column: 'cityCategory'\n",
    "#        Take 80% from 'B' and 20% from 'C'\n",
    "#          df.sampleBy('colName', fractions = {})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using verbs\n",
    ">select, <br>\n",
    "><i>select(x).where()</i>,<br>\n",
    "><i>select().distinct()</i>,<br>\n",
    ">filter,<br>\n",
    ">groupby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select syntax\n",
    "> DataFrame.select(\\*cols)<br>\n",
    "> cols: column names (string) or expressions (Column). If one of the column names is ‘*’, that column is expanded to include all columns in the current DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.0 Show columns 3rd till 5th\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter syntax\n",
    ">DataFrame.filter(condition)<br>\n",
    ">condition: <i>columnObject > 34</i> or string format: <i>\"age > 34\"</i>\n",
    ">>  df.age > 3 or col(\"age\") > 3<br>\n",
    ">>  \"age > 3\" <br>\n",
    ">>Logical Operators<br>\n",
    ">>> If string: AND OR NOT<br>\n",
    ">>> If columnObject: &, |, ~ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1 Filter purchases less than 9000\n",
    "#      Use filter()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.2 Filter purchases less than 9000 and maritalStatus is 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.3 Filter purchases less than 9000 or maritalStatus is 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.0 Combining verbs: select, filter and distinct\n",
    "#      select columns 'age' ,'purchase' \n",
    "#       filter purchaes for 'age' of 0-17\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation with groupby\n",
    "Use: <i>.agg({'colName1' : 'mean', 'colName2' : 'sum'})</i> <br>\n",
    ">With <i>agg()</i> one can use only builtin functions and not any other <i>pyspark.sql.function</i>.<br>\n",
    "Some common functions are: <i>mean, avg, sum, count, first, last,stddev </i>. There is no need to import builtin function in advance.<br>\n",
    "For a complete list of builtin functions see [here](https://sparkbyexamples.com/pyspark/pyspark-aggregate-functions/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find max of 'occupation' and max of 'purchase'\n",
    "\n",
    "blackfriday.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. groupby. Can apply sum, min, max, count\n",
    "\n",
    "\n",
    "\n",
    "# 12.1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### I am done ####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
