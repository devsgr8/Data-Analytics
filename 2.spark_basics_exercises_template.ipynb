{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Last amended:06th Sep, 2022\n",
    "#  Myfolder: /home/ashok/Documents/spark\n",
    "# Ref:\n",
    "# Tutorials (slightly dated):\n",
    "#      https://changhsinlee.com/pyspark-dataframe-basics/\n",
    "#      https://www.analyticsvidhya.com/blog/2016/10/spark-dataframe-and-operations/\n",
    "# Cheat Sheet\n",
    "#      https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_SQL_Cheat_Sheet_Python.pdf\n",
    "\n",
    "#  Objectives:\n",
    "#           Dataframe operations in spark cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyspark APIs<br>\n",
    "> i)  [DataFrame APIs](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#dataframe-apis)<br>\n",
    ">> df.select(columnName).where(colObject > 30).orderBy(desc(columnName))<br>\n",
    ">> df.select(columnName).where(\"colName > 30\").orderBy(desc(columnName))<br>\n",
    "\n",
    "> ii) [Column APIs](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#column-apis)<br>\n",
    ">> df.select(df.age.isNull())<br>\n",
    ">> df.select(df[\"age\"].isNull())<br>\n",
    ">> df.select(col(\"age\").isNull())<br>\n",
    "\n",
    "> iii)[Data Tyoes](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#data-types)<br>\n",
    "> iv) [Functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#functions)<br>\n",
    ">> df.select(sum(\"age\"))<br>\n",
    ">> df.select(sum(col(booleanColumn).cast(\"int\")))<br>\n",
    ">> <u>but you must import the functions</u>\n",
    "\n",
    "> v)  [Grouping](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#grouping)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A. Initial operations:\n",
    "1.0 Start hadoop in a terminal:\n",
    "\n",
    "            ./allstart.sh\n",
    "            OR\n",
    "            ./quick_allstart.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer files to hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/ashok/datadir\n",
      "Found 1 items\n",
      "-rw-r--r--   1 ashok supergroup    4582364 2022-09-06 15:46 /user/ashok/datadir/blackfridayless.csv\n"
     ]
    }
   ],
   "source": [
    "# 1.1 Transfer data file 'blackfridayless.csv' to hadoop\n",
    "#     Linux File folder:  /cdata/misc_datasets/black_friday\n",
    "#     In Hadoop first make a folder: /user/ashok/datadir \n",
    "#     and then transfer the file 'blackfridayless.csv' to \n",
    "#     this folder: /user/ashok/datadir\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "cd ~\n",
    "hdfs dfs -rm -f -r  /user/ashok/datadir\n",
    "hdfs dfs -mkdir /user/ashok/datadir\n",
    "hdfs dfs -put /cdata/misc_datasets/black_friday/blackfridayless.csv  /user/ashok/datadir\n",
    "hdfs dfs -ls /user/ashok/datadir\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "!cd ~\n",
    "!hdfs dfs -rm -f -r  /user/ashok/datadir\n",
    "!hdfs dfs -mkdir /user/ashok/datadir\n",
    "!hdfs dfs -put /cdata/misc_datasets/black_friday/blackfridayless.csv  /user/ashok/datadir\n",
    "!hdfs dfs -ls /user/ashok/datadir\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set jupyter notebook options\n",
    "Start pyspark with jupyter notebook interface. There is no need to create SparkContext and Spark session. pyspark creates them when starting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Display multiple outputs from a cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4454/2016363184.py:2: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1.3 Increase cell width to display wide columnar output\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the csv file from hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Read file 'blackfridayless.csv' from hadoop\n",
    "\n",
    "# 2.0 What is the URL of folder in hadoop where blackfriday file existshaving the file?\n",
    "#     url: \"http://localhost:9000/<folderPath>\"\n",
    "\n",
    "\n",
    "URL_of_folder= \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Read the file blackfridayless.csv. Takes time.\n",
    "#      Use 'spark.read.csv' session object to read file:\n",
    "#       Here is reading template:\n",
    "\n",
    "blackfriday = spark.read.csv(\n",
    "                             path = URL_of_folder + <filename> ,\n",
    "                             inferSchema =                  # True or False\n",
    "                             header =                       # True or False\n",
    "                             sep =                          # Which one: , ;, | etc\n",
    "                             ignoreLeadingWhiteSpace =      # True or False\n",
    "                             ignoreTrailingWhiteSpace =     # True of False\n",
    "    \n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Show five rows of data:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 Show data columns\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4 Show dtypes:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.5 Print schema of blackfriday:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.0 Describe the statistics of data, few columns at a time:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Count How many distinct userids are there \n",
    "#     Use distinct() and count()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Count how many distinct age-groups exist\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.0 How many null values occur in each column\n",
    "\n",
    "from pyspark.sql.functions import isnan, isnull,col, sum, max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1\n",
    "for i in blackfriday.columns:\n",
    "    blackfriday.select(sum(col(i).isNull().alias(\"nullcol\").cast(\"int\")).alias(i)).show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These columns have null values. Most probably it means that there is no sub-category or sub-categories present. <br>\n",
    "How would you plan to fill them?<br>\n",
    "productCat1 :  0 <br>\n",
    "productCat2 :  31429 <br>\n",
    "productCat3 :  70100 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.0 Get a list of all integer columns and string columns\n",
    "#     Use list comprhension along with dtypes:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Display maximum of productCat2 and productCat3\n",
    "#     Use select() along with 'max' function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 Find minimum and max values of 'occupation' column\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3 Fill null values in productCat2 and productCat3 with 999\n",
    "#     Use df.na.fill()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.0 Transform spark dataframe to pandas dataframe:\n",
    "#     Use df.toPandas()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Show a value count of levels of column 'cityCategory':\n",
    "#      Use groupby and count\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.0 Perform a stratified sampling of data.\n",
    "#      Stratified sampling be by column: 'cityCategory'\n",
    "#        Take 80% from 'B' and 20% from 'C'\n",
    "\n",
    "sample = blackfriday.sampleBy(\n",
    "                      \"cityCategory\",      # column that defines strata\n",
    "                      fractions = {'B' : 0.8, 'C' : 0.2})   # sampling fraction for each stratum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using verbs\n",
    ">select, <br>\n",
    "><i>select(x).where()</i>,<br>\n",
    "><i>select().distinct()</i>,<br>\n",
    ">filter,<br>\n",
    ">groupby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select syntax\n",
    "> DataFrame.select(\\*cols)<br>\n",
    "> cols: column names (string) or expressions (Column). If one of the column names is ‘*’, that column is expanded to include all columns in the current DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.0 Show columns 3rd till 5th\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter syntax\n",
    ">DataFrame.filter(condition)<br>\n",
    ">condition: <i>columnObject > 34</i> or string format: <i>\"age > 34\"</i>\n",
    ">>  df.age > 3 or col(\"age\") > 3<br>\n",
    ">>  \"age > 3\" <br>\n",
    ">>Logical Operators<br>\n",
    ">>> If string: AND OR NOT<br>\n",
    ">>> If columnObject: &, |, ~ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1 Filter purchases less than 9000\n",
    "#      Use filter()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.2 Filter purchases less than 9000 and maritalStatus is 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.3 Filter purchases less than 9000 or maritalStatus is 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.0 Combining verbs: select, filter and distinct\n",
    "\n",
    "airports_df.select('dst', 'tz'). \\\n",
    "            filter(airports_df.tz == -5). \\\n",
    "            show(3)\n",
    "\n",
    "# 10.1\n",
    "airports_df.select('dst', 'tz'). \\\n",
    "            filter(airports_df.tz == -5). \\\n",
    "            distinct(). \\\n",
    "            show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation with groupby\n",
    "Use: <i>.agg({'colName1' : 'mean', 'colName2' : 'sum'})</i> <br>\n",
    ">With <i>agg()</i> one can use only builtin functions and not any other <i>pyspark.sql.function</i>.<br>\n",
    "Some common functions are: <i>mean, avg, sum, count, first, last,stddev </i>. There is no need to import builtin function in advance.<br>\n",
    "For a complete list of builtin functions see [here](https://sparkbyexamples.com/pyspark/pyspark-aggregate-functions/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find max of 'maritalStatus' and max of 'purchase'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. groupby. Can apply sum, min, max, count\n",
    "\n",
    "airports_df.groupby('tz'). \\\n",
    "           count(). \\\n",
    "           show(3)\n",
    "\n",
    "# 12.1\n",
    "airports_df.groupby('tz'). \\\n",
    "            agg({'lat' : 'mean'}). \\\n",
    "            show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Unpacking operator in python (*) : \n",
    "Ref: https://codeyarns.com/2012/04/26/unpack-operator-in-python/ \n",
    "\n",
    "def fox(a,b):  \n",
    "    return (a *b)  \n",
    "\n",
    "m = [3,4]  \n",
    "fox(m)    \n",
    "fox(*m)   \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12.2 One can take the average of columns by passing\n",
    "#       an unpacked list of column names.\n",
    "\n",
    "grObject = airports_df.groupby('tz')\n",
    "\n",
    "avg_cols = ['lat', 'lon']\n",
    "grObject.avg(*avg_cols).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12.3 To call multiple aggregation functions at once, pass a dictionary.\n",
    "#         The 'key' of dictionary becomes argument to 'value'.\n",
    "#                             count(*)        avg(lat)      sum(lon)\n",
    "\n",
    "grObject.agg({'*': 'count', 'lat': 'avg', 'lon':'sum'}).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Create new columns in Spark using .withColumn() --mutate\n",
    "#      New column: altInThousands . \n",
    "#      Product of two columns:  'alt' and  'lon' \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Save the new file with additional column in parquet form\n",
    "\n",
    "xyz = airports_df.withColumn('altInThousands', airports_df.alt*airports_df.lon)\n",
    "xyz.write.parquet(\"hdfs://localhost:9000/user/ashok/data_files/airports_extra.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.1 Delete xyz from spark\n",
    "import gc\n",
    "del xyz\n",
    "gc.collect()    # Delete all cache also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.2 Read the stored parquet file\n",
    "df = spark.read.parquet(\"hdfs://localhost:9000/user/ashok/data_files/airports_extra.parquet\")\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.3 Read 'weather.csv file into spark from hadoop\n",
    "\n",
    "URL_of_file= \"hdfs://localhost:9000/user/ashok/data_files/nycflights/\"\n",
    "weather_df = spark.read.csv(path = URL_of_file + \"weather.csv\",\n",
    "                            inferSchema = True,\n",
    "                            header = True\n",
    "                           )\n",
    "weather_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Joins\n",
    "# Refer: http://www.learnbymarketing.com/1100/pyspark-joins-by-example/\n",
    "# For example, I can join the two titanic dataframes by the column PassengerId\n",
    "\n",
    "# 10.1\n",
    "airports_df.join(weather_df, airports_df.faa==weather_df.origin).show(3)\n",
    "# 10.2\n",
    "airports_df.join(weather_df, airports_df.faa==weather_df.origin, how = 'inner').show(3)\n",
    "# 10.3\n",
    "airports_df.join(weather_df, airports_df.faa==weather_df.origin, how = 'left').show(3)   # Could also use 'left_outer', 'right', 'full'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL queries against DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Many of the operations can be accessed by writing SQL queries in spark.sql().\n",
    "# To make an existing Spark dataframe usable for spark.sql(), one needs to\n",
    "#   register said dataframe as a temporary table.\n",
    "\n",
    "# 11.1 As an example, we can register the two dataframes as temp tables then\n",
    "#      join them through spark.sql().\n",
    "\n",
    "airports_df.createOrReplaceTempView('dfa_temp')\n",
    "weather_df.createOrReplaceTempView('dfw_temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11.2 Simple SQL query. SQLContext is no longer needed. 'spark'\n",
    "#            session object can be used.\n",
    "\n",
    "dfj = spark.sql('select * from dfa_temp' )\n",
    "dfj.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11.3 Now the SQL join\n",
    "\n",
    "dfj = spark.sql('select * from dfa_temp a, dfw_temp b where a.faa = b.origin' )\n",
    "dfj.show(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Drop a columns\n",
    "\n",
    "airports_df.drop('name').show(3)\n",
    "\n",
    "# 12.1  Or drop multiple columns\n",
    "\n",
    "columns_to_drop = ['name', 'lat']\n",
    "xx =airports_df.drop(*columns_to_drop)\n",
    "xx.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### I am done ####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
