{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last amended: 20th Sep, 2022\n",
    "# My folder: /home/ashok/Documents/spark/2.ml/1.demo\n",
    "##   Objectives:\n",
    "##  \t\ti)  Usage of StringIndexer, OneHotEncoder\n",
    "##              and VectorAssembler\n",
    "##          ii) Usage of pipelining \n",
    "##          iii) Data Transformations\n",
    "##\n",
    "##\n",
    "## Data: Save the following as: my.csv in \n",
    "##       /home/ashok/Documents/spark/ml/1.demo\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Sample of our datafile, my.csv:\n",
    "\n",
    "c1,c2,c3,n1,n2,n3,f\n",
    "a,x,1.0,0,2.1,3.2,y\n",
    "a,y,3.1,1,4.2,2.4,n\n",
    "b,y,1.1,0,1.4,2.5,y\n",
    "b,n,2.0,0,1.3,6.7,n\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer my.csv to hadoop, as:\n",
    "\n",
    "\n",
    "! hdfs dfs -rm hdfs://localhost:9000/user/ashok/my.csv\n",
    "#! hdfs dfs -put /home/ashok/Documents/spark/2.ml/1.demo/my.csv hdfs://localhost:9000/user/ashok\n",
    "! hdfs dfs -put /home/ashok/Downloads/my.csv  hdfs://localhost:9000/user/ashok\n",
    "#! hdfs dfs -cat  hdfs://localhost:9000/user/ashok/my.csv\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broad Steps\n",
    "1. Transform categorical data to integers (indices) using StringIndexer\n",
    "2. Transform indicies to OHE form\n",
    "3. Transform target seprately to integers (indices) using StrinIndexer\n",
    "4. Collect all numeric and OHE features in one place using VectorAssembler\n",
    "5. Perform modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small steps\n",
    "1. Transform categorical data to integers (indices) using StringIndexer\n",
    "> i) Create a list of categorical features<br>\n",
    ">ii) Create a StringIndexer object<br>\n",
    ">iii)Fit and transform using this object <br>\n",
    "\n",
    "2. Transform indices to OHE form<br>\n",
    ">i) Instantiate  an OHE object<br>\n",
    ">ii)Fit and transform indices createdas a result of 1(iii) above<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer files to hadoop\n",
    "Start hadoop and issue the following three commands\n",
    "\n",
    "```\n",
    "\n",
    "hdfs dfs -rm hdfs://localhost:9000/user/ashok/my.csv\n",
    "hdfs dfs -put /home/ashok/Documents/spark/2.ml/1.demo/my.csv hdfs://localhost:9000/user/ashok\n",
    "hdfs dfs -cat  hdfs://localhost:9000/user/ashok/my.csv\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1.0 Call libraries\n",
    "# 1.1   For transforming categorical data to integer and to dummy:\n",
    "#       And for collecting all features at one place\n",
    "\n",
    "from pyspark.ml.feature import  StringIndexer, OneHotEncoder, VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2   To execute all transformation operations as pipeline\n",
    "\n",
    "#from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 Logistic Regression modeling:\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4 For evaluating results:\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.0 Read demo data:\n",
    "\n",
    "df_demo = spark.read.csv(\n",
    "                         \"hdfs://localhost:9000/user/ashok/my.csv\",\n",
    "                          header = True,\n",
    "                          inferSchema = True\n",
    "                         ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Display data:\n",
    "\n",
    "df_demo.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1.1 Data type:\n",
    "\n",
    "df_demo.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Lists of string and numeric columns:\n",
    "\n",
    "cat_cols = ['c1','c2', 'c3']\t\n",
    "i_cols   = ['c11','c22', 'c33']             # Names after 'string indexing'\n",
    "\n",
    "num_cols = ['n1','n2','n3']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StrinIndex cat columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [What is a StringIndexer](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.StringIndexer.html)\n",
    ">StringIndexer is a label indexer that maps a string column of labels to an ML column of label indices. If the input column is numeric, we first cast it to string and then index the string values. The indices are in [0, numLabels). By default, this is ordered by label frequencies so the most frequent label gets index 0. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 Integer index string columns:\n",
    "\n",
    "# 2.3.1 Instantiate class:\n",
    "\n",
    "si     = StringIndexer(\n",
    "                        inputCols = cat_cols, \n",
    "                        outputCols = i_cols\n",
    "                       )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3.2 train StringIndexer object:\n",
    "\n",
    "model = si.fit(df_demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3.3 Let us see which levels have high frequencies:\n",
    "\n",
    "df_demo.groupby('c1').count().show()\n",
    "df_demo.groupby('c3').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3.4 Transform data and observe:\n",
    "\n",
    "df_demo = model.transform(df_demo)\n",
    "df_demo.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OneHotEncode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [What is OneHotEncoder](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.ml.feature.OneHotEncoder.html)\n",
    ">A one-hot encoder that maps a column of category indices to a column of binary vectors, with at most a single one-value per row that indicates the input category index. For example with 5 categories, an input value of 2.0 would map to an output vector of [0.0, 0.0, 1.0, 0.0]. The last category is not included by default (configurable via dropLast), because it makes the vector entries sum up to one, and hence linearly dependent. So an input value of 4.0 maps to [0.0, 0.0, 0.0, 0.0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.0 One hot encoding of indexed columns:\n",
    "\n",
    "ohe      = OneHotEncoder(\n",
    "                         inputCols = ['c11','c22','c33'],\n",
    "                         outputCols = ['c11vec','c22vec','c33vec']\n",
    "                         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 fit the data\n",
    "model_ohe = ohe.fit(df_demo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 transform the data\n",
    "df_demo = model_ohe.transform(df_demo)\n",
    "df_demo.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to interpret vectors?\n",
    "\n",
    "Consider the vector: <i>(48,[0, 1, 9],[14.1, 1.0, 1.0])</i>. This vector represents a vector of length 48, with three non-zero entries:\n",
    "\n",
    "    i)   14.1    at the 0th position\n",
    "    ii)  1.0     at the 1st position\n",
    "    iii) 1.0     at the 9th position\n",
    "    iv)  Rest all 45 positions would be 0.\n",
    "    \n",
    " Refer [here](https://stackoverflow.com/a/38236452)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StringIndex target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.0 indexing target separately\n",
    "#     Generally it is customary to name\n",
    "#     target as 'label'\n",
    "\n",
    "si_label = StringIndexer(\n",
    "                        inputCol = 'f',\n",
    "                        outputCol= 'label'\n",
    "                        ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1\n",
    "\n",
    "model_label = si_label.fit(df_demo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Transform dataframe:\n",
    "\n",
    "df_demo = model_label.transform(df_demo)\n",
    "df_demo.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Assembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.0 Using vectorassembler\n",
    "\n",
    "# 5.1 Create object\n",
    "#     Input cols are OHE columns + numerical columns\n",
    "#     Generally output col name is 'features'\n",
    "\n",
    "vc     = VectorAssembler(\n",
    "                          inputCols = ['c11vec','c22vec','c33vec', 'n1','n2','n3'],\n",
    "                          outputCol = 'features'\n",
    "                         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 vc_demo does not have 'fit' method\n",
    "#     only transform() is available\n",
    "#     So transform the data:\n",
    "\n",
    "df_demo = vc.transform(df_demo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.0 Instantiate Estimator:\n",
    "\n",
    "lr = LogisticRegression(\n",
    "                        labelCol=\"label\",\n",
    "                        featuresCol=\"features\",\n",
    "                        maxIter=10\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 fit the model\n",
    "lr_model = lr.fit(df_demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.0 Make predictions on df_demo \n",
    "#     itself using transform() method\n",
    "#     (There is no predict() method)\n",
    "\n",
    "predictions = lr_model.transform(df_demo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1 What columns are contained in the output\n",
    "predictions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.2 Select only relevant columns:\n",
    "\n",
    "selected = predictions.select(\"label\", \"prediction\", \"probability\", \"rawPrediction\")\n",
    "selected.show(truncate = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.  We can make use of the BinaryClassificationEvaluator method to\n",
    "#     evaluate our model.\n",
    "#     The Evaluator expects two input columns: (rawPrediction, label)\n",
    "#     and a value of 'metricName'\n",
    "#     By default -label- parameter has value 'label', 'metricName'\n",
    "#     has value of \"areaUnderROC\"\n",
    "\n",
    "# 9.1 Instantiate evaluate class\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "                                          rawPredictionCol=\"rawPrediction\"\n",
    "                                         )\n",
    "\n",
    "# 9.2 Evaluate to retun AUC\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################################\n",
    "######################################### Using Pipelining ######################################\n",
    "#################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StringIndexing and OHE pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Assemble all processing objects in a pipe\n",
    "#    and then use the pipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 list of stages:\n",
    "\n",
    "ToDo= [si ,ohe, si_label]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 Build pipe:\n",
    "\n",
    "pipe = Pipeline(stages=ToDo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3 Train pipe:\n",
    "\n",
    "model_pipe = pipe.fit(df_demo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.4 Transform data with pipe:\n",
    "\n",
    "df_trans = model_pipe.transform(df_demo)                       \n",
    "df_trans.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Assembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.0 Using vectorassembler\n",
    "\n",
    "# 6.1 Create object\n",
    "#     Input cols are OHE columns + numerical columns\n",
    "#     Generally output col name is 'features'\n",
    "\n",
    "vc     = VectorAssembler(\n",
    "                          inputCols = ['c11vec','c22vec','c33vec', 'n1','n2','n3'],\n",
    "                          outputCol = 'features'\n",
    "                         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 vc_demo does not have 'fit' method\n",
    "#     only transform() is available\n",
    "#     So transform the data:\n",
    "\n",
    "df_demo = vc.transform(df_demo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extending pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3 We add vc object to pipe\n",
    "\n",
    "ToDo= [si,ohe, si_label, vc]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.4 Instantiate Pipeline class:\n",
    "\n",
    "pipe = Pipeline(stages=ToDo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.5 Fit pipe:\n",
    "\n",
    "model_pipe = pipe.fit(df_demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.6 Transform data:\n",
    "\n",
    "df_trans = model_pipe.transform(df_demo)                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.7 Show:\n",
    "\n",
    "df_trans.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.0 Instantiate Estimator:\n",
    "\n",
    "lr = LogisticRegression(\n",
    "                        labelCol=\"label\",\n",
    "                        featuresCol=\"features\",\n",
    "                        maxIter=10\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final pipe with modeling\n",
    "What is a pipeline. Refer [here](https://spark.apache.org/docs/latest/ml-pipeline.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 Build final pipe and use it:\n",
    "\n",
    "ToDo= [si,ohe, si_label, vc, lr]\n",
    "pipe = Pipeline(stages=ToDo)\n",
    "lr_model = pipe.fit(df_demo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions and evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.0 Make predictions on df_demo \n",
    "#     itself using transform() method\n",
    "#     (There is no predict() method)\n",
    "\n",
    "predictions = lr_model.transform(df_demo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1\n",
    "predictions.columns        # All columns + 3 more\n",
    "                           # ['rawPrediction', 'probability', 'prediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.2 \n",
    "predictions.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.3 Select only relevant columns:\n",
    "\n",
    "selected = predictions.select(\"label\", \"prediction\", \"probability\", \"rawPrediction\")\n",
    "selected.show(truncate = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.  We can make use of the BinaryClassificationEvaluator method to\n",
    "#     evaluate our model.\n",
    "#     The Evaluator expects two input columns: (rawPrediction, label)\n",
    "#     and a value of 'metricName'\n",
    "#     By default -label- parameter has value 'label', 'metricName'\n",
    "#     has value of \"areaUnderROC\"\n",
    "\n",
    "# 9.1 Instantiate evaluate class\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "                                          rawPredictionCol=\"rawPrediction\"\n",
    "                                         )\n",
    "\n",
    "# 9.2 Evaluate to retun AUC\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.3  Note that the default metric for the\n",
    "#      BinaryClassificationEvaluator is areaUnderROC\n",
    "\n",
    "evaluator.getMetricName()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ I am done ################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 10.0 Basic Statistical operations\n",
    "#      Assemble only numerical data into a vector\n",
    "#      Many spark Statistical functions are available\n",
    "#      only on vector data. Here is an example how\n",
    "#      to use them.\n",
    "#      Ref: Basic Statistics\n",
    "#           https://spark.apache.org/docs/latest/ml-statistics.html#basic-statistics\n",
    "\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "vc_corr = VectorAssembler(\n",
    "                          inputCols = ['n1','n2','n3'],\n",
    "                          outputCol = 'vectors'\n",
    "                         )\n",
    "\n",
    "vec = vc_corr.transform(df_demo)\n",
    "\n",
    "r1 = Correlation.corr(vec, \"vectors\").head()\n",
    "print(\"Pearson correlation matrix:\\n\" + str(r1[0]))\n",
    "\n",
    "\n",
    "########################\n",
    "# Creating polynomial or interaction features\n",
    "########################\n",
    "# Extracting, transforming and selecting features\n",
    "#   Ref: https://spark.apache.org/docs/latest/ml-features#extracting-transforming-and-selecting-features\n",
    "# 11.0\n",
    "from pyspark.ml.feature import Interaction\n",
    "from pyspark.sql.functions import col                                                           \n",
    "\n",
    "# 11.1 VectorAssemble only numeric cols\n",
    "vc_num = VectorAssembler(\n",
    "                          inputCols = ['n1','n2','n3'],\n",
    "                          outputCol = \"vec\"\n",
    "                         )\n",
    "# 11.2\n",
    "df_trans = vc_num.transform(df_demo)\n",
    "\n",
    "# 11.3 Create a similar column to vector column\n",
    "df_trans = df_trans.withColumn(\"avec\", col(\"vec\"))\n",
    "\n",
    "# 12.0 Instantiate Interaction class\n",
    "poly= Interaction(\n",
    "                  inputCols=['avec','vec'],\n",
    "                  outputCol = 'features'\n",
    "                 )\n",
    "\n",
    "# 12.1 Transform and create features\n",
    "df_trans = poly.transform(df_trans)\n",
    "df_trans.select('vec','features').show(truncate = False)\n",
    "\n",
    "########################\n",
    "# 13.0 MinMaxScaling data\n",
    "########################\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scalerModel = scaler.fit(df_trans)\n",
    "df_trans = scalerModel.transform(df_trans)\n",
    "df_trans.show()\n",
    "########################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
